{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "print(len(text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 46, 53] hi ho\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(\"hi ho\")\n",
    "s = decode(tokens)\n",
    "print(tokens, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long, device = device)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]], device='cuda:0')\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size = 4, block_size = 8 ):\n",
    "    indices = torch.randint(len(data)-block_size-1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices], dim=0)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices], dim=0)\n",
    "    return x,y   \n",
    "\n",
    "x,y =  get_batch(train_data)    \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for i in range(100):   \n",
    "            x,y =  get_batch(dataset, 64, model.get_context_size())    \n",
    "            _, loss = model(x,y)\n",
    "            total += loss\n",
    "        model.train()\n",
    "        return float((total/100).cpu())\n",
    "\n",
    "def train(model, lr, batch_size, iterations, iter_eval, run_name=\"\"):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "\n",
    "    for it in range(iterations):\n",
    "        x,y =  get_batch(train_data, batch_size, model.get_context_size())    \n",
    "        _, loss = model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        if it % iter_eval == 0:\n",
    "            print(it//iter_eval, compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"./{model.model.name}_{run_name}.pth\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.name = f\"bigram_{vocab_size}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count):\n",
    "        s = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "        out = s\n",
    "        for i in range(count):\n",
    "            #print(s)\n",
    "            p, _ = self.forward(s)\n",
    "            probs = F.softmax(p[0], dim=1)\n",
    "            s = torch.multinomial(probs,1)\n",
    "            #print(\"sample\", ss)\n",
    "            out = torch.cat([out, s], dim=1)\n",
    "\n",
    "        return decode(out[0].tolist())\n",
    "    \n",
    "    def get_context_size(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "yq$;tfBfROkNdcuwdZZTkOMl;,ertK\n",
      "w:!PLCkMBbeA$3:XaSGJO-3p&M-c?KL3auhpFYVXJFhNNNuhq$OMxv.tbVFYdXlrFZaAeNuw:cPPyREFkHDEZaYJFzyWNuX\n",
      "Yo3&$LMtofBimzLB!!&V!Ox;Kl;l;ZcKe3 ixYeYEFngmi;;lxWvHFGEZEQG EsSXHB;kW3 J\n",
      "4.627649307250977 4.62070894241333\n",
      "0 4.639685153961182 4.646108150482178\n",
      "1 4.330250263214111 4.339630126953125\n",
      "2 4.061417579650879 4.086883544921875\n",
      "3 3.82150936126709 3.805820941925049\n",
      "4 3.6014459133148193 3.639233350753784\n",
      "5 3.446291446685791 3.463571310043335\n",
      "6 3.2874019145965576 3.3281590938568115\n",
      "7 3.202650308609009 3.2097079753875732\n",
      "8 3.050213575363159 3.106464147567749\n",
      "9 2.992992401123047 3.0160021781921387\n",
      "\n",
      "BYGENilerjbouselplind me l.\n",
      "lishe cnchiry:\n",
      "Uug;Mnisspllw y.O:ur n'SIREDmopetelivIEjMPithy wJd mothakllo W,Coo wh VCeiib3MI'Thom bMxWivDThenghim$Fs p-LK3gAY-xT3b\n",
      "\n",
      "ALENxmntcrurt f so;;3QQDLETm:\n",
      "EN,CI ma\n"
     ]
    }
   ],
   "source": [
    "bm = Generator(Bigram()).to(device)\n",
    "print(bm.generate(200))\n",
    "train(bm, lr=1e-3, batch_size=4, iterations=10000, iter_eval=1000)\n",
    "print(bm.generate(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, context_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        sz = context_size\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_query @ em_key.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        att += self.mask\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        return att @ em_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, context_size, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.head = nn.ModuleList( [Attention(context_size, embedding_size, embedding_size//num_heads) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        lx = self.ln1(x)\n",
    "        x1 = self.linear(torch.cat([head(lx) for head in self.head], dim=-1))\n",
    "        x1 = self.dp1(x1)\n",
    "        x = x + x1\n",
    "        \n",
    "        lx = self.ln2(x)\n",
    "        x2 = self.ff(lx)\n",
    "        x = x + x2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self, context_size, num_blocks, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "        self.name = f\"gpt_{context_size}_{num_blocks}_{num_heads}_{embedding_size}\"\n",
    "        self.context_size = context_size\n",
    "        pos = torch.arange(0, context_size, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.Sequential( *[Block(context_size, num_heads, embedding_size) for _ in range(num_blocks)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te + pe\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count, str=\" \"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.zeros((1,self.model.context_size), dtype=torch.long).to(device)\n",
    "\n",
    "            prompt = torch.tensor([encode(str)], dtype=torch.long, device = device)\n",
    "            prompt_len = len(str)\n",
    "\n",
    "            s[0, -prompt_len:] = prompt\n",
    "            out = s\n",
    "            for i in range(count):\n",
    "                p, _ = self.forward(out[:,-self.model.context_size:])\n",
    "                probs = F.softmax(p[0], dim=1)\n",
    "                s = torch.multinomial(probs,1)\n",
    "                out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "            return decode(out[0].tolist()[self.model.context_size-prompt_len:])\n",
    "        self.train()\n",
    "\n",
    "    def get_context_size(self):\n",
    "        return self.model.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 64):\n",
    "    print(\"configuration\", context_size, num_blocks, num_heads, embedding_size)\n",
    "    gen = ChatGPT(context_size, num_blocks, num_heads, embedding_size)\n",
    "    cg = Generator(gen).to(device)\n",
    "    print(gen.name+\".pth\")\n",
    "    try:\n",
    "        cg.load_state_dict(torch.load(gen.name+\".pth\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #train(cg, lr=1e-4, iterations=10)\n",
    "    return cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration 8 4 8 64\n",
      "gpt_8_4_8_64.pth\n",
      " UUJ$rA!xpD:: x;fDbRaxnxeWGsdGQ3qf3alANY!jtdogKW?':-cj$QN.Sia!nlkCn$x&OqCCxDNsa33 sPPu:KyYTg!D$UQ3ayF;:eDxqQa3x !Ed' an?McW$NfZF,xaKc$3cN&S'MYJ&f-QAc&Y$wxsUX$sf- IR.?'Bp$DUx3&snfcYl$-e\n",
      "qN3an$:bm tfrxJakN.OEYt3-?YXNeOqxowgpffQ&xcnva$bYk,mo-hh.JYDKnxxhkrNx',Ts3MY;KL$a!&-d j'L f?xYXcPadTT$xGxfmUXfk'ZjRxOagfoaqq!UH$f!QWIJ$xxkNroBNzYNWgYysQaefxLPfhfy,$eKVP$:ulczdfjxBXKEz$Dc$xfp cTv:;!!PWYMeDT-cYTfWlvkr,ckxx3.N\n",
      "VObWWfZx,NJmafcxXeaNul ;$Ha?YW:fbgdg'?soQ:-fPxMVflz3FfcBqyf-h!NfHfZ-fg3NUdV:\n",
      "GnxXd$ ;f&IzDkd\n",
      "4.343344211578369 4.341740131378174\n",
      "0 4.32589864730835 4.31722354888916\n",
      "1 3.397289276123047 3.4277865886688232\n",
      "2 3.1568245887756348 3.1844165325164795\n",
      "3 2.9406394958496094 2.953723907470703\n",
      "4 2.7771823406219482 2.796318292617798\n",
      "5 2.6856842041015625 2.6903133392333984\n",
      "6 2.621943712234497 2.6182382106781006\n",
      "7 2.581357002258301 2.5816054344177246\n",
      "8 2.5337867736816406 2.53439998626709\n",
      "9 2.496969699859619 2.5030245780944824\n",
      "10 2.463001012802124 2.4865808486938477\n",
      "11 2.4455833435058594 2.453223466873169\n",
      "12 2.423776149749756 2.434229612350464\n",
      "13 2.4067773818969727 2.4121010303497314\n",
      "14 2.384765625 2.3898000717163086\n",
      "15 2.376775026321411 2.377520799636841\n",
      "16 2.354113817214966 2.363765001296997\n",
      "17 2.336850881576538 2.3512279987335205\n",
      "18 2.335359811782837 2.3407764434814453\n",
      "19 2.3156867027282715 2.311302661895752\n",
      " che wheachs beres ind desiNe ars and,\n",
      "Edesy whave litt oy pins eand fand thary waicj:\n",
      "WhodsWr'hs, wour wist kerdonin\n",
      "Tit lo dedend!\n",
      "\n",
      "The the'd toy;\n",
      "Uher\n",
      "Ituther to cout ind;\n",
      "Is, leped usts faditdll? to ven cen, rise as inat, toert banwe\n",
      "qoutson ame\n",
      "Besenk bris,\n",
      "I po? rilecPreans mtesp,\n",
      "Tfordsif hhe mpronde the fprand to por to orre mod ang,\n",
      "ENom bed:\n",
      "Simele toe lSeresse aas,\n",
      "KI Kevess bong st to dea you ou thirdd Fhind thet uem thereren.\n",
      "\n",
      "LALrercepppoancy astard mene thit brortitredea habrobel f\n"
     ]
    }
   ],
   "source": [
    "e = Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 8*8)\n",
    "print(e.generate(500))\n",
    "train(e, lr=1e-4, batch_size=64, iterations=2000, iter_eval=100 )\n",
    "print(e.generate(500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration 256 6 6 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_256_6_6_384.pth\n",
      "4.3363566398620605 4.341015338897705\n",
      "0 3.981755018234253 3.9955132007598877\n",
      "1 2.6786558628082275 2.700760841369629\n",
      "2 2.5303521156311035 2.542194366455078\n",
      "3 2.4951159954071045 2.506218910217285\n",
      "4 2.474208116531372 2.488203287124634\n",
      "5 2.4575226306915283 2.475525140762329\n",
      "6 2.441577911376953 2.459533452987671\n",
      "7 2.423408031463623 2.4427058696746826\n",
      "8 2.4047064781188965 2.427353620529175\n",
      "9 2.3934988975524902 2.4105567932128906\n",
      "10 2.35853910446167 2.377129077911377\n",
      "11 2.3211984634399414 2.347334146499634\n",
      "12 2.280453681945801 2.3132970333099365\n",
      "13 2.2331511974334717 2.2614188194274902\n",
      "14 2.178341865539551 2.2184650897979736\n",
      "15 2.136789560317993 2.1865761280059814\n",
      "16 2.0976104736328125 2.14700984954834\n",
      "17 2.058650016784668 2.1214194297790527\n",
      "18 2.029344320297241 2.0925214290618896\n",
      "19 1.9949886798858643 2.0658798217773438\n",
      "20 1.9657407999038696 2.050616979598999\n",
      "21 1.9388052225112915 2.0284903049468994\n",
      "22 1.913833737373352 2.0178563594818115\n",
      "23 1.8914642333984375 2.0059235095977783\n",
      "24 1.8614099025726318 1.9783178567886353\n",
      "25 1.846766471862793 1.9668257236480713\n",
      "26 1.8220014572143555 1.9433321952819824\n",
      "27 1.8006975650787354 1.9307667016983032\n",
      "28 1.7832865715026855 1.9203741550445557\n",
      "29 1.764617681503296 1.906126856803894\n",
      "30 1.7488808631896973 1.88804030418396\n",
      "31 1.7269915342330933 1.8709880113601685\n",
      "32 1.7154486179351807 1.870962142944336\n",
      "33 1.6980069875717163 1.8559281826019287\n",
      "34 1.682715892791748 1.8441224098205566\n",
      "35 1.670908808708191 1.836329460144043\n",
      "36 1.6579458713531494 1.8195797204971313\n",
      "37 1.6458545923233032 1.8163715600967407\n",
      "38 1.6315304040908813 1.8042259216308594\n",
      "39 1.6213346719741821 1.7957888841629028\n",
      "40 1.6122890710830688 1.7900604009628296\n",
      "41 1.597644329071045 1.7744946479797363\n",
      "42 1.5939092636108398 1.7774077653884888\n",
      "43 1.582880973815918 1.7574715614318848\n",
      "44 1.578874945640564 1.7590813636779785\n",
      "45 1.5666637420654297 1.7541084289550781\n",
      "46 1.5551694631576538 1.7379761934280396\n",
      "47 1.545108437538147 1.7317320108413696\n",
      "48 1.537617802619934 1.7248727083206177\n",
      "49 1.5291842222213745 1.723099946975708\n",
      "50 1.5187923908233643 1.7109489440917969\n",
      "51 1.5137073993682861 1.7122082710266113\n",
      "52 1.5102567672729492 1.7070649862289429\n",
      "53 1.5050654411315918 1.703324794769287\n",
      "54 1.4976530075073242 1.6939212083816528\n",
      "55 1.487705945968628 1.688313603401184\n",
      "56 1.4782203435897827 1.684096336364746\n",
      "57 1.473813772201538 1.681947946548462\n",
      "58 1.4725044965744019 1.6733232736587524\n",
      "59 1.4613375663757324 1.6683319807052612\n",
      " CLYORIX him.\n",
      "\n",
      "SABELANut:\n",
      "AN\n",
      "DUCKENC;\n",
      "IUchame had though olon be rear.\n",
      "\n",
      "HENRY BOLINGBOKE:\n",
      "That ormorman my lord.\n",
      "\n",
      "BENVOLIO:\n",
      "She come to do Promean think you dear,\n",
      "And pear the grune in shall full these homer\n",
      "That has you ever my knower Lord:\n",
      "And, 'pless' that of the morust\n",
      "Ag youn and not; for forwicter to formand,\n",
      "In Havinestious, Lord sunderhing it,\n",
      "That know deper they seems to thee offence are.\n",
      "Romeo, nor shall come thy duke fally own that trrick,\n",
      "With is love sprinquith, which the father.\n",
      "\n",
      "PXINIA:\n",
      "My life's grull before;\n",
      "Play does him in hath sword:\n",
      "And But life you sen his bown:\n",
      "This I must sun, and what I she wo those;\n",
      "For meturns, thou hast strowland of him;\n",
      "And is, there crowful; it then gract, ood my lord:\n",
      "I'll no sucher till the may;\n",
      "And aludins foundrests trusgly he; there,\n",
      "Hother shall should me of queen thee.\n",
      "\n",
      "Messengper:\n",
      "Uscardard, hair will!\n",
      "\n",
      "GLOUCESTER:\n",
      "I proclours to do the call with is country?\n",
      "See tending lifes honour head do.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "But fould with down of risce?\n",
      "\n",
      "GLOUCESTER:\n",
      "Stand no thing.\n",
      "\n",
      "PEIS't:\n",
      "This is heavI head.\n",
      "\n",
      "BENVOLIO:\n",
      "\n",
      "SICINIUS:\n",
      "Sould comes;\n",
      "And when, but this boy woman such be here,\n",
      "And the dream the king do glipings\n",
      "For were newfore-broked their later less.\n",
      "\n",
      "KING RICHARD III:\n",
      "Come, I the write out oghy.\n",
      "\n",
      "KING RICHARD III:\n",
      "Thou darked my leave long lord; no pape\n",
      "Never for Talural obless the have cheep\n",
      "Aga for the rone.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Plantate joy rot this spince of put!\n",
      "Come here give you and hand sens a bleaved,\n",
      "My crave anam the c\n"
     ]
    }
   ],
   "source": [
    "e2 = Experiment(context_size = 256, num_blocks = 6, num_heads = 6, embedding_size = 6*64)\n",
    "train(e2, lr=1e-4, batch_size=64, iterations=3000, iter_eval=50 )\n",
    "print(e2.generate(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting test\n",
    "e3 = Experiment(context_size = 256, num_blocks = 6, num_heads = 6, embedding_size = 6*64)\n",
    "train(e3, lr=1e-4, batch_size=64, iterations=15000, iter_eval=100, run_name = \"overfitting\" )\n",
    "print(e3.generate(500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
