{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx+1:idx + self.block_size + 1]\n",
    "        return x, y   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"dataset\": \"tinyshakespeare\",\n",
    "    \"dropout\": 0.20,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"context_size\": 256,\n",
    "    \"num_blocks\": 6, \n",
    "    \"num_heads\": 6, \n",
    "    \"iter_eval\": 200,\n",
    "    \"embedding_size\": 6*64\n",
    "}\n",
    "\n",
    "locals().update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 1, 56, 43,  ...,  1, 59, 57],\n",
      "        [58, 46, 56,  ..., 51, 11,  1],\n",
      "        [ 1, 53, 44,  ..., 41, 46,  1],\n",
      "        ...,\n",
      "        [59, 52, 58,  ..., 58,  1, 56],\n",
      "        [ 1, 58, 59,  ..., 53, 44,  1],\n",
      "        [ 1, 39, 57,  ...,  1, 44, 39]], device='cuda:0')\n",
      "tensor([[56, 43, 52,  ..., 59, 57,  2],\n",
      "        [46, 56, 59,  ..., 11,  1, 52],\n",
      "        [53, 44,  1,  ..., 46,  1, 44],\n",
      "        ...,\n",
      "        [52, 58, 43,  ...,  1, 56, 53],\n",
      "        [58, 59, 56,  ..., 44,  1, 51],\n",
      "        [39, 57, 57,  ..., 44, 39, 47]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long, device = device)\n",
    "n = int(0.9*len(data))\n",
    "training_set = CustomDataset(data[:n], context_size)\n",
    "validation_set  = CustomDataset(data[n:], context_size)\n",
    "    \n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)  \n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "for i, (x,y) in enumerate(training_generator):\n",
    "    print(i)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "\n",
    "def save_model(model, run_name=\"\"):\n",
    "    torch.save(model.state_dict(), f\"./{model.model.name}_{run_name}.pth\")    \n",
    "\n",
    "def load_model(model, run_name=\"\"):    \n",
    "    model.load_state_dict(torch.load(f\"./{model.model.name}_{run_name}.pth\"))\n",
    "\n",
    "def compute_loss(model, generator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for it, (x,y) in enumerate(generator):\n",
    "            if it>100:\n",
    "                break\n",
    "            _, loss = model(x,y)\n",
    "            total += loss\n",
    "        model.train()\n",
    "        return float((total/100).cpu())\n",
    "\n",
    "def train(model, config = {}, notes = \"\", tags = []):\n",
    "\n",
    "    config[\"model_name\"] = model.model.name\n",
    "\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.login()\n",
    "\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"myChatGPT\",\n",
    "            notes=notes,\n",
    "            tags=tags,        \n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config = config\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(compute_loss(model, training_generator), compute_loss(model, validation_generator))\n",
    "\n",
    "    for it, (x,y) in enumerate(training_generator):\n",
    "        _, loss = model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        if it % iter_eval == 0:\n",
    "            train_loss = compute_loss(model, training_generator)\n",
    "            val_loss = compute_loss(model, validation_generator)            \n",
    "            if use_wandb:\n",
    "                wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss})\n",
    "            else:\n",
    "                print(it//iter_eval, train_loss, val_loss)\n",
    "        \n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, context_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        sz = context_size\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_query @ em_key.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        att += self.mask\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        return att @ em_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.head = nn.ModuleList( [Attention(context_size, embedding_size, embedding_size//num_heads) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        lx = self.ln1(x)\n",
    "        x1 = self.linear(torch.cat([head(lx) for head in self.head], dim=-1))\n",
    "        x1 = self.dp1(x1)\n",
    "        x = x + x1\n",
    "        \n",
    "        lx = self.ln2(x)\n",
    "        x2 = self.ff(lx)\n",
    "        x = x + x2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(0, context_size, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.Sequential( *[Block(context_size, num_heads, embedding_size) for _ in range(num_blocks)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te + pe\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count, str=\" \"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.zeros((1,self.model.context_size), dtype=torch.long).to(device)\n",
    "\n",
    "            prompt = torch.tensor([encode(str)], dtype=torch.long, device = device)\n",
    "            prompt_len = len(str)\n",
    "\n",
    "            s[0, -prompt_len:] = prompt\n",
    "            out = s\n",
    "            for i in range(count):\n",
    "                p, _ = self.forward(out[:,-self.model.context_size:])\n",
    "                probs = F.softmax(p[0], dim=1)\n",
    "                s = torch.multinomial(probs,1)\n",
    "                out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "            return decode(out[0].tolist()[self.model.context_size-prompt_len:])\n",
    "        self.train()\n",
    "\n",
    "    def get_context_size(self):\n",
    "        return self.model.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ChatGPT.__init__() takes 1 positional argument but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mChatGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m cg \u001b[38;5;241m=\u001b[39m Generator(gen)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#cg.load_state_dict(torch.load(gen.name+\".pth\"))\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ChatGPT.__init__() takes 1 positional argument but 5 were given"
     ]
    }
   ],
   "source": [
    "cg = Generator(ChatGPT()).to(device)\n",
    "#cg.load_state_dict(torch.load(gen.name+\".pth\"))\n",
    "train(cg, config, notes = \"overfit\", tags=[] )\n",
    "print(cg.generate(1500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
