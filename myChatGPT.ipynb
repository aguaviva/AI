{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3a1817b050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "print(len(text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 46, 53] hi ho\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(\"hi ho\")\n",
    "s = decode(tokens)\n",
    "print(tokens, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[43,  1, 49, 47, 52, 45,  1, 56],\n",
      "        [50, 53, 57, 43,  1, 58, 46, 43],\n",
      "        [59,  1, 39, 56, 58,  1, 61, 53],\n",
      "        [56, 57, 43,  1, 46, 39, 60, 43]])\n",
      "tensor([[ 1, 49, 47, 52, 45,  1, 56, 43],\n",
      "        [53, 57, 43,  1, 58, 46, 43,  1],\n",
      "        [ 1, 39, 56, 58,  1, 61, 53, 51],\n",
      "        [57, 43,  1, 46, 39, 60, 43,  1]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(data):\n",
    "    indices = torch.randint(len(data)-block_size-1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices], dim=0)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices], dim=0)\n",
    "    return x,y   \n",
    "\n",
    "x,y =  get_batch(train_data)    \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, iterations):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for j in range(iterations):\n",
    "        for i in range(1000):   \n",
    "            x,y =  get_batch(train_data)    \n",
    "            _, loss = model(x,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "        print(loss.item())\n",
    "\n",
    "    torch.save(model.state_dict(), \"./mymodel.pth\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        #lx = F.one_hot(x.reshape(-1), vocab_size).type(torch.float32)\n",
    "        #print(lx.dtype, lx.shape)\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count):\n",
    "        s = torch.zeros((1,1), dtype=torch.long)\n",
    "        out = s\n",
    "        for i in range(count):\n",
    "            #print(s)\n",
    "            p, _ = self.forward(s)\n",
    "            probs = F.softmax(p[0], dim=1)\n",
    "            s = torch.multinomial(probs,1)\n",
    "            #print(\"sample\", ss)\n",
    "            out = torch.cat([out, s], dim=1)\n",
    "\n",
    "        return decode(out[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3cnIFf.,u?e' cvOu?!&YXdDbn?&fffWtxvef,;bn'r$xWpnbUazN!YSaq3vTipGhJkTWR:G3!harUEe'bnzSENC;UjTF\n",
      ":'dvgXXKZElpJPsDbkXdv\n",
      "3ZAAgrAg$$jgOdN:i:HqOvZhFFNNLm3aQiuMrUi.xDY!T E;gnl, iOf'$;OwqXmQJdVcbJ,m'.rIocFP.;S\n",
      "3.894418954849243\n",
      "3.5484983921051025\n",
      "3.3535237312316895\n",
      "3.000521659851074\n",
      "2.7436137199401855\n",
      "2.645657777786255\n",
      "2.6030640602111816\n",
      "2.6691739559173584\n",
      "2.526980400085449\n",
      "2.479642868041992\n",
      "\n",
      "MyQzhe!, do t ar MbedK:\n",
      "HEShind r.f ts ckioca lin sause.\n",
      "DYremawock\n",
      "AUn as RGvinengongis\n",
      "TO:\n",
      "Lnd imyo,\n",
      "JIn d wige me fr'ss grm.\n",
      "WshOZ$CKAngahe Gwim, ayarr ard e cdend mngronsFlad thin t ft:\n",
      "\n",
      "F-s.\n",
      "\n",
      "O, \n"
     ]
    }
   ],
   "source": [
    "\n",
    "bm = Generator(Bigram())\n",
    "print(bm.generate(200))\n",
    "train(bm, lr=1e-3, iterations=10)\n",
    "print(bm.generate(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_key @ em_query.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        sz = att.shape[-1]\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        att += mask\n",
    "\n",
    "        att = F.softmax(att, dim=1)\n",
    "        return att @ em_value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.0\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        attention_size = embedding_size//num_heads\n",
    "        self.head = nn.ModuleList( [Attention(embedding_size, attention_size) for _ in range(num_heads)])\n",
    "\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        x = x + torch.cat([head(x) for head in self.head], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dp1(x)\n",
    "\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ff(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self, num_blocks, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos = torch.arange(0, block_size, dtype=torch.long)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(block_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.ModuleList( [Block(num_heads, embedding_size) for _ in range(num_blocks)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te+pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "c = ChatGPT(4,8,8*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        #lx = F.one_hot(x.reshape(-1), vocab_size).type(torch.float32)\n",
    "        #print(lx.dtype, lx.shape)\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count):\n",
    "        s = torch.zeros((1,8), dtype=torch.long)\n",
    "        out = s\n",
    "        for i in range(count):\n",
    "            #print(s)\n",
    "            p, _ = self.forward(out[:,-8:])\n",
    "            probs = F.softmax(p[0], dim=1)\n",
    "            s = torch.multinomial(probs,1)\n",
    "            #print(\"sample\", ss)\n",
    "            out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "        return decode(out[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42737531661987305\n"
     ]
    }
   ],
   "source": [
    "#cg = Generator(ChatGPT(4,8,64))\n",
    "train(cg, lr=1e-4, iterations=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stwhalite of is pe'dsend;\n",
      "Aly enppevot nor acunkbecospis nosthe fey nicn alos met ave; oen. mese fareme:szeg,\n",
      "It,\n",
      "Thit do It entsent,\n",
      "Sand!\n",
      "'d ceres'dPWif aness of an!be oftse-taly geor astenuglhU:reves not sony tiv: la?\n",
      "Heprad elsteat te.\n",
      "\n",
      "I'rdpok isg incadetse!s?\n",
      "Seger corangatcs lohate\n",
      "CIbpay bogod to tae,'s fosent,\n",
      "Fingsegporit at,n nce anfanginn,\n",
      "Tigt whonge ant inete int lakcannd,\n",
      "Go rorosengo uren Hencest enocer;\n",
      "Ram to to dsal' seWitecncanontte, imes,\n",
      "Mical,\n",
      "I est' the at, istiate the sind itest ian.\n",
      "\n",
      "Drise cadant fory,\n",
      "dt to meise and ant, aveing!\n",
      "Nongeasssceb oro fat odrseen ok coonente, ind asenlord minnsas sonecn faveantelea yo ancond ancu .\n",
      "\n",
      "Yat oren hale, cand to my sat-O\n",
      "Itsetls ripowears;\n",
      "ewen! jut aveelt nor morat aint:\n",
      "Yoseensd\n",
      "eonuhinggelsf, pavessimek,, ke ached aenleent otl-ed\n",
      "Had\n",
      "And a manc,\n",
      "Youdtinsang, esson ske are lave.\n",
      "\n",
      "G ihat at moynst avhthsingefced sanke youse nat and tend, aso an no sago steis a faverche!\n",
      "OUNr incakdeng larsinn athere, chagre ku pen sansonte k we,\n",
      "IevHo,\n",
      "Thpde ite bot  ysast:\n",
      "Iest,\n",
      "We\n",
      "o an owgrous ind s&o ntence  agn inogndtest, pindsost bebudaem, mes and te?\n",
      "\n",
      "Sbet astan ono so nendse,'els:ke sate ecse., ad mbave;\n",
      "Wtabt teen inscse,'n?\n",
      ":\n",
      "Io and taat anlongte wectreso wesunnd conken anst lub ancugenggiccensg! ans a sincrese,\n",
      "Thenelnn aln le thecuselr teviss uatesuse nyomn'd ome.\n",
      "\n",
      "IY ofs:\n",
      "Uey so non temelrenot .\n",
      "\n",
      "Dees-?\n",
      "Non yo te, en coonckeck homf to cuind thad nagcay.\n",
      "\n",
      "If me'dt wh walad ofnd itit he sso Mo, a avisoitlete.\n",
      "\n",
      "CAULINT at no at veend att oorn iceTen, pigh.\n",
      "\n",
      "UNy ateso\n",
      "Titsu enullsatellad\n",
      "I:\n",
      "Mat'l; deld to pacamee\n",
      "ang fintengse rorsond satinltat stinlo ispeng;\n",
      "Thy ou alifs; tion tantt aneams,\n",
      "\n",
      "I Mate To cy,ssat hade send isecat me'singelto EU's masn,.\n",
      "\n",
      "gleves, lase.\n",
      "\n",
      "salsor ift,\n",
      "os nocsp akoded\n",
      "Mulbrleene\n",
      "Twilssay,\n",
      "We bitr ace to in tonscenst o tearsencere,\n",
      "wing anle ang, aslites fagnatelvce'e santan y, noreen do-sre ancoreng.\n",
      "\n",
      "O:\n",
      "StREs :ilad int to suse te es oo'c non 'elf;well n aune;\n",
      "Igcovesrede dorod.\n",
      "\n",
      ":\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = torch.zeros((1,8), dtype=torch.long)\n",
    "out = s\n",
    "for i in range(2000):\n",
    "    #print(s)\n",
    "    p, _ = cg(out[:,-8:])\n",
    "    probs = F.softmax(p[0], dim=1)\n",
    "    s = torch.multinomial(probs,1)\n",
    "    #print(\"sample\", ss)\n",
    "    out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "print( decode(out[0].tolist()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
