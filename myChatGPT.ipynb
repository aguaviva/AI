{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx+1:idx + self.block_size + 1]\n",
    "        return x, y   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"dataset\": \"tinyshakespeare\",\n",
    "    \"dropout\": 0.20,\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"context_size\": 256,\n",
    "    \"num_blocks\": 6, \n",
    "    \"num_heads\": 6, \n",
    "    \"iter_eval\": 200,\n",
    "    \"embedding_size\": 6*64\n",
    "}\n",
    "\n",
    "locals().update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[ 1, 56, 43,  ...,  1, 59, 57],\n",
      "        [58, 46, 56,  ..., 51, 11,  1],\n",
      "        [ 1, 53, 44,  ..., 41, 46,  1],\n",
      "        ...,\n",
      "        [59, 52, 58,  ..., 58,  1, 56],\n",
      "        [ 1, 58, 59,  ..., 53, 44,  1],\n",
      "        [ 1, 39, 57,  ...,  1, 44, 39]], device='cuda:0')\n",
      "tensor([[56, 43, 52,  ..., 59, 57,  2],\n",
      "        [46, 56, 59,  ..., 11,  1, 52],\n",
      "        [53, 44,  1,  ..., 46,  1, 44],\n",
      "        ...,\n",
      "        [52, 58, 43,  ...,  1, 56, 53],\n",
      "        [58, 59, 56,  ..., 44,  1, 51],\n",
      "        [39, 57, 57,  ..., 44, 39, 47]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long, device = device)\n",
    "n = int(0.9*len(data))\n",
    "training_set = CustomDataset(data[:n], context_size)\n",
    "validation_set  = CustomDataset(data[n:], context_size)\n",
    "    \n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)  \n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "for i, (x,y) in enumerate(training_generator):\n",
    "    print(i)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "\n",
    "def save_model(model, run_name=\"\"):\n",
    "    torch.save(model.state_dict(), f\"./{model.model.name}_{run_name}.pth\")    \n",
    "\n",
    "def load_model(model, run_name=\"\"):    \n",
    "    model.load_state_dict(torch.load(f\"./{model.model.name}_{run_name}.pth\"))\n",
    "\n",
    "def compute_loss(model, generator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for it, (x,y) in enumerate(generator):\n",
    "            if it>100:\n",
    "                break\n",
    "            _, loss = model(x,y)\n",
    "            total += loss\n",
    "        model.train()\n",
    "        return float((total/100).cpu())\n",
    "\n",
    "def train(model, config = {}, notes = \"\", tags = []):\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        wandb.login()\n",
    "\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"myChatGPT\",\n",
    "            notes=notes,\n",
    "            tags=tags,        \n",
    "\n",
    "            # track hyperparameters and run metadata\n",
    "            config = config\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(compute_loss(model, training_generator), compute_loss(model, validation_generator))\n",
    "\n",
    "    for it, (x,y) in enumerate(training_generator):\n",
    "        _, loss = model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        if it % iter_eval == 0:\n",
    "            train_loss = compute_loss(model, training_generator)\n",
    "            val_loss = compute_loss(model, validation_generator)            \n",
    "            if use_wandb:\n",
    "                wandb.log({\"val_loss\": val_loss, \"train_loss\": train_loss})\n",
    "            else:\n",
    "                print(it//iter_eval, train_loss, val_loss)\n",
    "        \n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, context_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        sz = context_size\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_query @ em_key.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        att += self.mask\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        return att @ em_value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.head = nn.ModuleList( [Attention(context_size, embedding_size, embedding_size//num_heads) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        lx = self.ln1(x)\n",
    "        x1 = self.linear(torch.cat([head(lx) for head in self.head], dim=-1))\n",
    "        x1 = self.dp1(x1)\n",
    "        x = x + x1\n",
    "        \n",
    "        lx = self.ln2(x)\n",
    "        x2 = self.ff(lx)\n",
    "        x = x + x2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pos = torch.arange(0, context_size, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.Sequential( *[Block() for _ in range(num_blocks)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te + pe\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count, str=\" \"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.zeros((1,self.model.context_size), dtype=torch.long).to(device)\n",
    "\n",
    "            prompt = torch.tensor([encode(str)], dtype=torch.long, device = device)\n",
    "            prompt_len = len(str)\n",
    "\n",
    "            s[0, -prompt_len:] = prompt\n",
    "            out = s\n",
    "            for i in range(count):\n",
    "                p, _ = self.forward(out[:,-self.model.context_size:])\n",
    "                probs = F.softmax(p[0], dim=1)\n",
    "                s = torch.multinomial(probs,1)\n",
    "                out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "            return decode(out[0].tolist()[self.model.context_size-prompt_len:])\n",
    "        self.train()\n",
    "\n",
    "    def get_context_size(self):\n",
    "        return self.model.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.411170482635498 4.409477233886719\n",
      "0 4.036060333251953 4.029866695404053\n",
      "1 2.4990503787994385 2.501516103744507\n",
      "2 2.425372838973999 2.435488700866699\n",
      "3 2.278076648712158 2.2900524139404297\n",
      "4 2.1061394214630127 2.1247808933258057\n",
      "5 1.9775710105895996 2.0132534503936768\n",
      "6 1.8825962543487549 1.9319041967391968\n",
      "7 1.7967580556869507 1.8509390354156494\n",
      "8 1.7337689399719238 1.7905222177505493\n",
      "9 1.6807433366775513 1.743286371231079\n",
      "10 1.6332776546478271 1.6977356672286987\n",
      "11 1.5949636697769165 1.6607798337936401\n",
      "12 1.5589029788970947 1.6348291635513306\n",
      "13 1.529707908630371 1.6026982069015503\n",
      "14 1.5034362077713013 1.5828063488006592\n",
      "15 1.4778387546539307 1.5594685077667236\n",
      "16 1.4547239542007446 1.533329725265503\n",
      "17 1.4363317489624023 1.5218416452407837\n",
      "18 1.4178204536437988 1.5051500797271729\n",
      "19 1.4016895294189453 1.4817265272140503\n",
      "20 1.386785864830017 1.46378755569458\n",
      "21 1.3721859455108643 1.4469170570373535\n",
      "22 1.3543869256973267 1.44060218334198\n",
      "23 1.3416690826416016 1.4271981716156006\n",
      "24 1.3311196565628052 1.412048578262329\n",
      "25 1.3227490186691284 1.4116936922073364\n",
      "26 1.3095115423202515 1.394921064376831\n",
      "27 1.2989895343780518 1.3897472620010376\n",
      "28 1.2913892269134521 1.381407618522644\n",
      "29 1.2790892124176025 1.3753018379211426\n",
      "30 1.267467737197876 1.366478443145752\n",
      "31 1.2588855028152466 1.3673977851867676\n",
      "32 1.256447196006775 1.360870599746704\n",
      "33 1.244149923324585 1.3508319854736328\n",
      "34 1.235882043838501 1.3508964776992798\n",
      "35 1.2272491455078125 1.342734456062317\n",
      "36 1.2178515195846558 1.3398789167404175\n",
      "37 1.213404893875122 1.3387216329574585\n",
      "38 1.2054903507232666 1.3349533081054688\n",
      "39 1.19646418094635 1.331211805343628\n",
      "40 1.190887451171875 1.3237775564193726\n",
      "41 1.1804783344268799 1.3273791074752808\n",
      "42 1.1732274293899536 1.3254873752593994\n",
      "43 1.1670798063278198 1.3197256326675415\n",
      "44 1.1575322151184082 1.3271243572235107\n",
      "45 1.151309609413147 1.3186848163604736\n",
      "46 1.1468846797943115 1.3229856491088867\n",
      "47 1.1382472515106201 1.3172099590301514\n",
      "48 1.1314648389816284 1.315483570098877\n",
      "49 1.1258041858673096 1.3166544437408447\n",
      "50 1.1171902418136597 1.3146013021469116\n",
      "51 1.1111267805099487 1.3203049898147583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cg \u001b[38;5;241m=\u001b[39m Generator(ChatGPT())\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#cg.load_state_dict(torch.load(gen.name+\".pth\"))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cg\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m1500\u001b[39m))\n",
      "Cell \u001b[0;32mIn[17], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, notes, tags)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()    \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m%\u001b[39m iter_eval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m compute_loss(model, validation_generator)            \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(model, generator)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m it\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/vfi_training/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 9\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         ly \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(y, vocab_size)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.conda/envs/vfi_training/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mChatGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m pe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m     53\u001b[0m x \u001b[38;5;241m=\u001b[39m te \u001b[38;5;241m+\u001b[39m pe\n\u001b[0;32m---> 55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m~/.conda/envs/vfi_training/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/vfi_training/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/vfi_training/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     lx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n\u001b[0;32m---> 24\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdp1(x1)\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cg = Generator(ChatGPT()).to(device)\n",
    "#cg.load_state_dict(torch.load(gen.name+\".pth\"))\n",
    "train(cg, config, notes = \"overfit\", tags=[] )\n",
    "print(cg.generate(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
