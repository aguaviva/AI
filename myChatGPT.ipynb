{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "print(len(text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 46, 53] hi ho\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(\"hi ho\")\n",
    "s = decode(tokens)\n",
    "print(tokens, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long, device = device)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size = 4, block_size = 8 ):\n",
    "    indices = torch.randint(len(data)-block_size-1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices], dim=0)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices], dim=0)\n",
    "    return x,y   \n",
    "\n",
    "x,y =  get_batch(train_data)    \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for i in range(100):   \n",
    "            x,y =  get_batch(train_data, 64, model.get_context_size())    \n",
    "            _, loss = model(x,y)\n",
    "            total += loss\n",
    "        model.train()\n",
    "        return float((total/100).cpu())\n",
    "\n",
    "def train(model, lr, batch_size, iterations, iter_eval):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "\n",
    "    for it in range(iterations):\n",
    "        x,y =  get_batch(train_data, batch_size, model.get_context_size())    \n",
    "        _, loss = model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        if it % iter_eval == 0:\n",
    "            print(it//iter_eval, compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "        \n",
    "    torch.save(model.state_dict(), f\"./{model.model.name}.pth\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Embedding(vocab_size, vocab_size)\n",
    "        self.name = f\"bigram_{vocab_size}\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count):\n",
    "        s = torch.zeros((1,1), dtype=torch.long, device = device)\n",
    "        out = s\n",
    "        for i in range(count):\n",
    "            #print(s)\n",
    "            p, _ = self.forward(s)\n",
    "            probs = F.softmax(p[0], dim=1)\n",
    "            s = torch.multinomial(probs,1)\n",
    "            #print(\"sample\", ss)\n",
    "            out = torch.cat([out, s], dim=1)\n",
    "\n",
    "        return decode(out[0].tolist())\n",
    "    \n",
    "    def get_context_size(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "phIOWd3AqNcgg,G!;j\n",
      "UtVYwJteWJc3xq.NBpFdLXaqK; eyjnB,Icl'Vn3M3M:JSe;bVbN N&DsRi?!DSaeyNZlSYjVkCzkSdocO'f rCrC'co&sLpCaOlmvxIRlq;-nspc;kDlMHulz.BnzPOI$ISUvRZe;oWJ ?!TxUj!!wTcIFx$G!Zvm,CruxtJoMZjPUZcXtLM\n",
      "4.69345235824585 4.660988807678223\n",
      "0 4.700205326080322 4.691677570343018\n",
      "1 4.355329990386963 4.346986293792725\n",
      "2 4.058333873748779 4.076082706451416\n",
      "3 3.8295791149139404 3.8127782344818115\n",
      "4 3.617060899734497 3.602130174636841\n",
      "5 3.4610977172851562 3.4833998680114746\n",
      "6 3.2942519187927246 3.3329217433929443\n",
      "7 3.1765172481536865 3.201218366622925\n",
      "8 3.1023147106170654 3.0953824520111084\n",
      "9 3.0085549354553223 3.0099878311157227\n",
      "\n",
      "Wr-FWJcKd.ug yasLUTEQwPZDxQifRUct'ssondobatelayo dkvT3n gqCray Ye LEmennden vofevANleyxugroeer! nd plBAD.zmme?!c oms\n",
      "AQPnPoaky\n",
      "Rbre mon aFL.xraJNOBoret slegrusoulyZDWIIU'HW'ey wnzvkelujhy sh.ary wixVY\n"
     ]
    }
   ],
   "source": [
    "bm = Generator(Bigram()).to(device)\n",
    "print(bm.generate(200))\n",
    "train(bm, lr=1e-3, batch_size=4, iterations=10000, iter_eval=1000)\n",
    "print(bm.generate(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, context_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        sz = context_size\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_query @ em_key.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        att += self.mask\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        return att @ em_value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.1\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, context_size, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.head = nn.ModuleList( [Attention(context_size, embedding_size, embedding_size//num_heads) for _ in range(num_heads)])\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        lx = self.ln1(x)\n",
    "        x1 = self.linear(torch.cat([head(lx) for head in self.head], dim=-1))\n",
    "        x1 = self.dp1(x1)\n",
    "        x = x + x1\n",
    "        \n",
    "        lx = self.ln2(x)\n",
    "        x2 = self.ff(lx)\n",
    "        x = x + x2\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self, context_size, num_blocks, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "        self.name = f\"gpt_{context_size}_{num_blocks}_{num_heads}_{embedding_size}\"\n",
    "        self.context_size = context_size\n",
    "        pos = torch.arange(0, context_size, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.Sequential( *[Block(context_size, num_heads, embedding_size) for _ in range(num_blocks)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te + pe\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count, str=\" \"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.zeros((1,self.model.context_size), dtype=torch.long).to(device)\n",
    "\n",
    "            prompt = torch.tensor([encode(str)], dtype=torch.long, device = device)\n",
    "            prompt_len = len(str)\n",
    "\n",
    "            s[0, -prompt_len:] = prompt\n",
    "            out = s\n",
    "            for i in range(count):\n",
    "                p, _ = self.forward(out[:,-self.model.context_size:])\n",
    "                probs = F.softmax(p[0], dim=1)\n",
    "                s = torch.multinomial(probs,1)\n",
    "                out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "            return decode(out[0].tolist()[self.model.context_size-prompt_len:])\n",
    "        self.train()\n",
    "\n",
    "    def get_context_size(self):\n",
    "        return self.model.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 64):\n",
    "    print(\"configuration\", context_size, num_blocks, num_heads, embedding_size)\n",
    "    gen = ChatGPT(context_size, num_blocks, num_heads, embedding_size)\n",
    "    cg = Generator(gen).to(device)\n",
    "    #train(cg, lr=1e-4, iterations=10)\n",
    "    return cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration 8 4 8 64\n",
      " eGp\n",
      "ncRaIC$czv:QtGPTnsM!q,n;gdDxnwggnNdFlTGgHRqqKQxn,ynkRs\n",
      "$CRVeS,nvHng3l$olz$IUqKzigCnyVVneNcft?PxeNgq,nHg!BsQu,EA3nTe;MMIdOGDGnAxsyr.nFCqLg gGik,cUax,nyp'V!oe$jGgVvBHRuL;,aNDxR3GPWwRokbFT:SndQ$rEWjueYxeDxvQ,ZOdF,G;slxD3awiI?sZDn-gg\n",
      "xdu'Bcav'qCZMsycDIsBeNX\n",
      "nMkT,U$d!Vk&eGXx;t\n",
      "ayBkRnOCnCTRsbOC,:R?NV;lDwNJF$EEvF&rhIqyDeq$h&pXgOyBsXBnnOdC& CnQ;Cj:xebsYEuIFqxzxjTeYNwOwfuqGZVG,xFMnFd3KNizxn\n",
      "!Tb,:&$$y,VpBZB:xRj$TSOzh!!IcNECW kQzvCui:$i NiIRzDQhLRyIN?WEkPFX?RcmPGRxfX!aIzdQsuq'3cy3nYbsbRIE&qmegwnH:EyghV\n",
      "4.4329142570495605 4.432741165161133\n",
      "0 4.3880391120910645 4.388359546661377\n",
      "1 3.350644826889038 3.352203607559204\n",
      "2 2.9748764038085938 2.960547685623169\n",
      "3 2.765075922012329 2.7615926265716553\n",
      "4 2.624894142150879 2.638394355773926\n",
      "5 2.5509159564971924 2.5549302101135254\n",
      "6 2.5041286945343018 2.492727756500244\n",
      "7 2.4508743286132812 2.4369056224823\n",
      "8 2.40779185295105 2.3942034244537354\n",
      "9 2.3743042945861816 2.3805015087127686\n",
      "10 2.3426871299743652 2.338502883911133\n",
      "11 2.3084938526153564 2.3067879676818848\n",
      "12 2.287130832672119 2.2771615982055664\n",
      "13 2.2584657669067383 2.2414464950561523\n",
      "14 2.2420318126678467 2.222449779510498\n",
      "15 2.2243239879608154 2.2270395755767822\n",
      "16 2.1966590881347656 2.202065944671631\n",
      "17 2.2002105712890625 2.177008628845215\n",
      "18 2.169708490371704 2.1569764614105225\n",
      "19 2.159586191177368 2.153204917907715\n",
      " hiss goum aing sen: batied as thou drewer unads,\n",
      "Come toukee.\n",
      "\n",
      "IO wu\n",
      "War$th. I IPity bitok?\n",
      "\n",
      "BLALCELE:\n",
      "To man chaceEend ofmord ingpatinginces hateverth now bowdshard woum,\n",
      "If won stet of arled if Head to manists is not.-\n",
      "\n",
      "BLINO!\n",
      "Posterlund.\n",
      "\n",
      "ROSJOERK:\n",
      "\n",
      "Watherle?\n",
      "\n",
      "KI:\n",
      "Lorthced:\n",
      "kond seak hid sucdles hing,om,\n",
      "Andis thimlle lemet with. I \n",
      "Clopt thiry.\n",
      "\n",
      "KIONP'peZak athe by ust profpe to to made monceent\n",
      "own sucy our mse bemrencordirghtaond on warnouns konks but fairnd be tollCion ttare henords, hard\n"
     ]
    }
   ],
   "source": [
    "e = Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 8*8)\n",
    "print(e.generate(500))\n",
    "train(e, lr=1e-4, batch_size=64, iterations=2000, iter_eval=100 )\n",
    "print(e.generate(500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = Experiment(context_size = 256, num_blocks = 6, num_heads = 6, embedding_size = 6*64)\n",
    "train(e2, lr=1e-4, batch_size=64, iterations=2000, iter_eval=50 )\n",
    "print(e2.generate(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hcha\n",
      "Shsthe me\n",
      "oomy thadr cer:\n",
      "\n",
      "Ns bo;e-ve wolhgsi ils ut:\n",
      "LsAr theefirertou the thime wly ardik shisT loug Yhseelne fre aale wepirsunt nol feomu twwilr-edo thit mat al th pd chane Yoparf That wy dfifr of, tae thal! bole Oj whenter hit o linet, noirtig fwirntor hop,\n",
      " was.\n",
      "\n",
      "l'ndsd gu e of!ingus ht, outhrharge tidtegiyenen eas tkee Aarw,: on md woer\n",
      "'gdeve gat thirgy ht, sons, A-hrealt, dherofarexeu fy f\n",
      "heralg, one will\n",
      "We scbatousSh wilonar rLukthrerq.\n",
      "R:\n",
      "COUSW Kd enrel, hseintotat shanldl lnoc ornels cnes thehrelfiere p afu karus oury ald omet vente oel k wopn m owy oso omie wd wyor.\n",
      "H batce pore co-e se twatat whe firc k bin foprsns nork.\n",
      "OENRUNNSRLRERAETE:\n",
      "CF:\n",
      "LCEENyxe,\n",
      "Noirtoes ooro vuechenod is binsort tatn wors h! vafst, oind,\n",
      "Wat gerasry se no thmy on.\n",
      "ULNi w youe th stheou the mion, dhres\n",
      "ON:\n",
      "LW3ain.\n",
      "ORPET\n",
      "Tha, ad terhy dals wher thu the my tharpe-se ger aend drigiktr els lefh-\n",
      "wege  deehrer pbraket gsle wor he esy dace, fh ofwr'd forif, ben bhe\n",
      "Ri've fow ord we wel  abl theori amlnolo thir otilghornible an mlon trolllgrerdr brGourgat thainst ethshu amey, ore buny t s'lreraimren, caretire sogrte goard cy;rye, le banb,\n",
      "Athd\n",
      "EURKEOSESUUHWe\n",
      "AeTalhd ROAENR:\n",
      "Beu \n",
      "fm,te f naf ant wn afmous t wyurein th de aA b'bit thues toret, fpisvant He,s ero rense borde ter, yoo lsey wyournt, ardr fuarlselt, homisit oh manses oeldd cingut arrok. lorIsals huleely wisto we'l forat oloy weu bawerto, tharginy opkr;ow,\n",
      "Tha thurrife, tu fars teel sat o cith mips intse fr hana nor echat ktr w a\n"
     ]
    }
   ],
   "source": [
    "#train(e, lr=1e-4, iterations=2000, iter_eval=100 )\n",
    "print(e.generate(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT(\n",
      "  (tok_embedding): Embedding(65, 384)\n",
      "  (pos_embedding): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): ModuleList(\n",
      "        (0-5): 6 x Attention(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dp1): Dropout(p=0.1, inplace=False)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ff): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gen = ChatGPT(context_size=256, num_blocks=6, num_heads=6, embedding_size=6*64)\n",
    "print(gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
