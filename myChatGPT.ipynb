{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from  torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "cuda = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open(\"input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "print(len(text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "print(\"\".join(vocab))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 46, 53] hi ho\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "ctoi = { vocab[i]:i for i in range(len(vocab))}\n",
    "itoc = { i:vocab[i] for i in range(len(vocab))}\n",
    "def encode(s): return [ ctoi[i] for i in s]\n",
    "def decode(t): return \"\".join([ itoc[i] for i in t])\n",
    "\n",
    "tokens = encode(\"hi ho\")\n",
    "s = decode(tokens)\n",
    "print(tokens, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56]\n",
      "First Citizen:\n",
      "Befor\n"
     ]
    }
   ],
   "source": [
    "tokens = encode(text)\n",
    "print(tokens[:20])\n",
    "print(decode(tokens[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(tokens, dtype=torch.long, device = cuda)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
      "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
      "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
      "        [ 1, 39,  1, 46, 53, 59, 57, 43]], device='cuda:0')\n",
      "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
      "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
      "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
      "        [39,  1, 46, 53, 59, 57, 43,  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size = 4, block_size = 8 ):\n",
    "    indices = torch.randint(len(data)-block_size-1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in indices], dim=0)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in indices], dim=0)\n",
    "    return x,y   \n",
    "\n",
    "x,y =  get_batch(train_data)    \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        for i in range(100):   \n",
    "            x,y =  get_batch(train_data, 64, model.get_context_size())    \n",
    "            _, loss = model(x,y)\n",
    "            total += loss\n",
    "        model.train()\n",
    "        return float((total/100).cpu())\n",
    "\n",
    "def train(model, lr, batch_size, iterations, iter_eval):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "\n",
    "    for it in range(iterations):\n",
    "        x,y =  get_batch(train_data, batch_size, model.get_context_size())    \n",
    "        _, loss = model(x,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        if it % iter_eval == 0:\n",
    "            print(it//iter_eval, compute_loss(model, train_data), compute_loss(model, val_data))\n",
    "        \n",
    "    torch.save(model.state_dict(), \"./mymodel.pth\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count):\n",
    "        s = torch.zeros((1,1), dtype=torch.long, device = cuda)\n",
    "        out = s\n",
    "        for i in range(count):\n",
    "            #print(s)\n",
    "            p, _ = self.forward(s)\n",
    "            probs = F.softmax(p[0], dim=1)\n",
    "            s = torch.multinomial(probs,1)\n",
    "            #print(\"sample\", ss)\n",
    "            out = torch.cat([out, s], dim=1)\n",
    "\n",
    "        return decode(out[0].tolist())\n",
    "    def get_context_size(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "? .vf&EuQDBg'PCWo!KzqdX:KGB&E$YxUPoGi'lwW;&U CzqS'KfjZ?X-VwDmVw&cXazZC$Dk:wG-L!.Z?GuFAzYuYy!CgCz;.!RNgQ!I;-nseOs,mJ$a.hdXqV:$PDo3tS\n",
      ",&JWrUPR paJVnNiAmIsVf&.IMD$n'Ipf&p!A3d$YM,;kE3;l,MKkGhviLxQ!oM\n",
      ".UPu\n",
      "4.652835369110107 4.6526336669921875\n",
      "0 4.63444709777832 4.641069412231445\n",
      "1 4.309333801269531 4.295302391052246\n",
      "2 4.044012546539307 4.044595718383789\n",
      "3 3.8016421794891357 3.8051838874816895\n",
      "4 3.5808169841766357 3.57332181930542\n",
      "5 3.4287338256835938 3.411273717880249\n",
      "6 3.2615296840667725 3.275895833969116\n",
      "7 3.143493890762329 3.1579220294952393\n",
      "8 3.0518107414245605 3.057640314102173\n",
      "9 2.9937422275543213 2.9483277797698975\n",
      "\n",
      "s;IKWlane.Nl t,\n",
      "Mkeng?MXG$PSL&Tve ystinilgrOf.hGrymo ngYKm fasltcy s,e,\n",
      "AJX twhte eerd\n",
      "BGr cURurld;.FCisenookpe w;I't Okxl SeILZo pSLOTuJL?dBambH\n",
      "BBTSCredow\n",
      "GTy',wimismifaLHNue\n",
      ".\n",
      "Rpuo inC: fo ymio twa\n"
     ]
    }
   ],
   "source": [
    "bm = Generator(Bigram()).cuda()\n",
    "print(bm.generate(200))\n",
    "train(bm, lr=1e-3, batch_size=4, iterations=10000, iter_eval=1000)\n",
    "print(bm.generate(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, context_size, input_size, output_size):\n",
    "        super().__init__()\n",
    "        # KQV size\n",
    "        self.output_size = output_size\n",
    "        self.key = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, output_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, output_size, bias=False)\n",
    "\n",
    "        sz = context_size\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        em_key = self.key(x)\n",
    "        em_query = self.query(x)\n",
    "        em_value = self.value(x)\n",
    "\n",
    "        # the attentions matrix must be the size of the context\n",
    "        # as it is in reality an adjacency matrix\n",
    "        att = em_key @ em_query.transpose(-2,-1)\n",
    "\n",
    "        #print (att.shape)\n",
    "\n",
    "        att /= self.output_size ** 0.5\n",
    "\n",
    "        att += self.mask\n",
    "\n",
    "        att = F.softmax(att, dim=1)\n",
    "        return att @ em_value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.1\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, context_size, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        attention_size = embedding_size//num_heads\n",
    "        self.head = nn.ModuleList( [Attention(context_size, embedding_size, attention_size) for _ in range(num_heads)])\n",
    "\n",
    "        self.linear = nn.Linear(embedding_size, embedding_size)\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_size, embedding_size),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        x = x + torch.cat([head(x) for head in self.head], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dp1(x)\n",
    "\n",
    "        x = self.ln2(x)\n",
    "        x = x + self.ff(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChatGPT(nn.Module):\n",
    "    def __init__(self, context_size, num_blocks, num_heads, embedding_size):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        pos = torch.arange(0, context_size, dtype=torch.long)\n",
    "        self.register_buffer(\"pos\", pos)\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.pos_embedding = nn.Embedding(context_size, embedding_size)\n",
    "\n",
    "        self.blocks = nn.ModuleList( [Block(context_size, num_heads, embedding_size) for _ in range(num_blocks)])\n",
    "        self.ff = nn.Sequential(nn.Linear(embedding_size,embedding_size), nn.ReLU())\n",
    "\n",
    "        self.ln = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        te = self.tok_embedding(x)\n",
    "        pe = self.pos_embedding(self.pos)\n",
    "        x = te+pe\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        p = self.model(x)\n",
    "        if y!=None:\n",
    "            ly = F.one_hot(y, vocab_size).type(torch.float32)\n",
    "            loss = loss_fn(p.permute(0,2,1), ly.permute(0,2,1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return p, loss\n",
    "    \n",
    "    def generate(self, count, str=\" \"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            s = torch.zeros((1,self.model.context_size), dtype=torch.long).cuda()\n",
    "\n",
    "            prompt = torch.tensor([encode(str)], dtype=torch.long, device = cuda)\n",
    "            prompt_len = len(str)\n",
    "\n",
    "            s[0, -prompt_len:] = prompt\n",
    "            out = s\n",
    "            for i in range(count):\n",
    "                p, _ = self.forward(out[:,-self.model.context_size:])\n",
    "                probs = F.softmax(p[0], dim=1)\n",
    "                s = torch.multinomial(probs,1)\n",
    "                out = torch.cat([out, s[-1].unsqueeze(1)], dim=1)\n",
    "\n",
    "            return decode(out[0].tolist()[self.model.context_size-prompt_len:])\n",
    "        self.train()\n",
    "\n",
    "    def get_context_size(self):\n",
    "        return self.model.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 64):\n",
    "    print(\"configuration\", context_size, num_blocks, num_heads, embedding_size)\n",
    "    gen = ChatGPT(context_size, num_blocks, num_heads, embedding_size)\n",
    "    cg = Generator(gen).cuda()\n",
    "    #train(cg, lr=1e-4, iterations=10)\n",
    "    return cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration 8 4 8 64\n",
      " dk-soUrPESe !RCGz?dJvrkKMkeWDCHPyzoZYu YkHB'i,iQQtSag,edCadBDOYYOQMKCDMwGwBLtlYCmd .-,D;l nXl,oCRiZKXb;xWVsRQHd;z!KfJA'fJGQzjAYECyr3,zfYa!r,KOGK,\n",
      "VOYvkfHBVzKJ?r.M  t-!ZcYOlBXqJyA3bWk$u:!JOoCRD3-ZSSCeD S,\n",
      ":S3exDVYeUD,rHSqeAOvY:rOCS&kia'c:lQy&SHFXgWAuk'CbFH-hSZHAvqCC3kCkjbSrrbrjNOC$ZJ-KeZ&bXAHELcZKvZhZLcGkr$3J'e?SG oKJZxC\n",
      "3DELYrDJdcYh\n",
      "BdAbKmmAGPxAUYH!SGSMtaKSmD zpUwVFG.MGx$ YBia.AtSG!XvmXSMPbSdrEPLevZY;OLJJvFYDpzYYv !3Hmr-ADkcQELY?dfzJQHHJz3OevJC!J.y D.VLwU-E xwYRGLLi CpMlvMxK-vkTAPZZKjfJcFD\n",
      "p'OBh\n",
      "4.437148571014404 4.432563781738281\n",
      "0 4.40033483505249 4.396738052368164\n",
      "1 3.428243637084961 3.4338808059692383\n",
      "2 3.150041103363037 3.154521942138672\n",
      "3 2.9211373329162598 2.9315361976623535\n",
      "4 2.709534168243408 2.7067461013793945\n",
      "5 2.4750494956970215 2.4850308895111084\n",
      "6 2.2734384536743164 2.2822346687316895\n",
      "7 2.0735151767730713 2.070044755935669\n",
      "8 1.8975770473480225 1.8976020812988281\n",
      "9 1.7318187952041626 1.7282023429870605\n",
      "10 1.5803191661834717 1.5726232528686523\n",
      "11 1.4260536432266235 1.4200739860534668\n",
      "12 1.2883886098861694 1.2915074825286865\n",
      "13 1.169974684715271 1.1747232675552368\n",
      "14 1.0634467601776123 1.0655752420425415\n",
      "15 0.9615789651870728 0.9565806984901428\n",
      "16 0.8808150887489319 0.8751359581947327\n",
      "17 0.8312022089958191 0.8209210634231567\n",
      "18 0.7584013342857361 0.7686455845832825\n",
      "19 0.7151691913604736 0.7301227450370789\n",
      " thakald, want w hlakin min rarensryntilat m of amserriritre iptif vatd.  wrericoeratutheitn hat y'rine t berad enr d lklod so iset des thard\n",
      "To dimreregy cu than the fhate era nfiera ke binh w&il be tsueead esI yomirger tolesef ind keire feer yenoernt, tis pin h'sive mip\n",
      "L\n",
      "RUSEd:\n",
      "Tarf, oe sr,it,si thih, mer elhitr wikts sdot ous b poor whetr Sond m'g, butat fe:\n",
      "Goontes, thil thather the he koBve perd suWtirss stou\n",
      "Aopn meil,L kernisu e'slrorO cel, nofdreat teigo,\n",
      "eonud e pon thd the tharer hot b\n"
     ]
    }
   ],
   "source": [
    "e = Experiment(context_size = 8, num_blocks = 4, num_heads = 8, embedding_size = 8*8)\n",
    "print(e.generate(500))\n",
    "train(e, lr=1e-4, batch_size=64, iterations=2000, iter_eval=100 )\n",
    "print(e.generate(500))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration 256 6 6 384\n",
      "4.423750877380371 4.423376560211182\n",
      "0 3.834313154220581 3.834627866744995\n",
      "1 2.518970012664795 2.5189764499664307\n",
      "2 2.417795419692993 2.418952226638794\n",
      "3 2.047351598739624 2.046271562576294\n",
      "4 1.5564768314361572 1.5556992292404175\n",
      "5 1.0190346240997314 1.0173932313919067\n",
      "6 0.608237624168396 0.6081591844558716\n",
      "7 0.34533897042274475 0.34360986948013306\n",
      "8 0.19945649802684784 0.200038880109787\n",
      "9 0.11963575333356857 0.12120773643255234\n",
      "10 0.08075428009033203 0.08019108325242996\n",
      "11 0.07577800005674362 0.0764923170208931\n",
      "12 0.04499192163348198 0.04530977085232735\n",
      "13 0.03225255012512207 0.03188958764076233\n",
      "14 0.02567167952656746 0.025795895606279373\n",
      "15 0.02208765409886837 0.022337250411510468\n",
      "16 0.020169099792838097 0.019875528290867805\n",
      "17 0.017486875876784325 0.01775313727557659\n",
      "18 0.01560223288834095 0.015622979961335659\n",
      "19 0.015707334503531456 0.015536091290414333\n",
      " ICKHYYTDdDTHCWDOBDO:\n",
      "SRAPE sIL:\n",
      "Ahee, sfeeer tat atheo -Nit,\n",
      "HoUEANhINAvIErU:\n",
      "\n",
      "Ss\n",
      "IB:\n",
      "ViN the bois Iis sroo, doue hrifandrO:\n",
      "F,et shat tr:radnir watc ifnid y emakn woess, tot,\n",
      "Tcor?\n",
      "M:\n",
      "gome tomn.\n",
      "Thot bos:\n",
      "Bot wer memlr as cre s  shiinr klltE\n",
      ".LUN AGNI\n",
      "CrCkTAN&A:\n",
      "B;\n",
      "IMS:\n",
      "Set sre,\n",
      ": asw s nura epet I boonr rere wand het,\n",
      "Erl essobd hinrey\n",
      "Is alor mounf Pe the anromei,\n",
      "\n",
      "I Shanrce yenfe wa do, bovredou,we\n",
      "y pothand ipean a'un the tyor loe fond dono ho qds wt Poenct hendl doner nered wh.\n",
      "CoLOL thier\n"
     ]
    }
   ],
   "source": [
    "e2 = Experiment(context_size = 256, num_blocks = 6, num_heads = 6, embedding_size = 6*64)\n",
    "train(e2, lr=1e-4, batch_size=64, iterations=2000, iter_eval=100 )\n",
    "print(e2.generate(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hcha\n",
      "Shsthe me\n",
      "oomy thadr cer:\n",
      "\n",
      "Ns bo;e-ve wolhgsi ils ut:\n",
      "LsAr theefirertou the thime wly ardik shisT loug Yhseelne fre aale wepirsunt nol feomu twwilr-edo thit mat al th pd chane Yoparf That wy dfifr of, tae thal! bole Oj whenter hit o linet, noirtig fwirntor hop,\n",
      " was.\n",
      "\n",
      "l'ndsd gu e of!ingus ht, outhrharge tidtegiyenen eas tkee Aarw,: on md woer\n",
      "'gdeve gat thirgy ht, sons, A-hrealt, dherofarexeu fy f\n",
      "heralg, one will\n",
      "We scbatousSh wilonar rLukthrerq.\n",
      "R:\n",
      "COUSW Kd enrel, hseintotat shanldl lnoc ornels cnes thehrelfiere p afu karus oury ald omet vente oel k wopn m owy oso omie wd wyor.\n",
      "H batce pore co-e se twatat whe firc k bin foprsns nork.\n",
      "OENRUNNSRLRERAETE:\n",
      "CF:\n",
      "LCEENyxe,\n",
      "Noirtoes ooro vuechenod is binsort tatn wors h! vafst, oind,\n",
      "Wat gerasry se no thmy on.\n",
      "ULNi w youe th stheou the mion, dhres\n",
      "ON:\n",
      "LW3ain.\n",
      "ORPET\n",
      "Tha, ad terhy dals wher thu the my tharpe-se ger aend drigiktr els lefh-\n",
      "wege  deehrer pbraket gsle wor he esy dace, fh ofwr'd forif, ben bhe\n",
      "Ri've fow ord we wel  abl theori amlnolo thir otilghornible an mlon trolllgrerdr brGourgat thainst ethshu amey, ore buny t s'lreraimren, caretire sogrte goard cy;rye, le banb,\n",
      "Athd\n",
      "EURKEOSESUUHWe\n",
      "AeTalhd ROAENR:\n",
      "Beu \n",
      "fm,te f naf ant wn afmous t wyurein th de aA b'bit thues toret, fpisvant He,s ero rense borde ter, yoo lsey wyournt, ardr fuarlselt, homisit oh manses oeldd cingut arrok. lorIsals huleely wisto we'l forat oloy weu bawerto, tharginy opkr;ow,\n",
      "Tha thurrife, tu fars teel sat o cith mips intse fr hana nor echat ktr w a\n"
     ]
    }
   ],
   "source": [
    "#train(e, lr=1e-4, iterations=2000, iter_eval=100 )\n",
    "print(e.generate(1500))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
