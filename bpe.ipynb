{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.myChatGPT import *\n",
    "from src.metrics import *\n",
    "#from src.exp_weighted_pairs import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text=f.read()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(vocab_size, data):\n",
    "    return torch.concatenate([data[:-1].unsqueeze(1),data[1:].unsqueeze(1)], dim=1) \n",
    "\n",
    "def generate_bigrams_id(vocab_size, data):\n",
    "    return generate_bigrams(vocab_size, data) @ torch.tensor([1,vocab_size])\n",
    "\n",
    "def count_sequence_of_bigrams(vocab_size, data):\n",
    "    a = generate_bigrams_id(vocab_size, data)\n",
    "    return torch.zeros(vocab_size*vocab_size, dtype=torch.long).scatter_add(0, a, torch.ones_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.vocab = list(set(text))\n",
    "        self.vocab.sort()\n",
    "\n",
    "        self.ctoi = { self.vocab[i]:i for i in range(len(self.vocab)) }\n",
    "        self.itoc = { i:self.vocab[i] for i in range(len(self.vocab)) }\n",
    "\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def encode(self, s): \n",
    "        data = torch.tensor([ self.ctoi[i] for i in s ], dtype=torch.long)\n",
    "        return data\n",
    "\n",
    "    def decode(self, t): \n",
    "        return \"\".join([ self.itoc[i] for i in t ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56,  ..., 45,  8,  0]) ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "568208 ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'e ', 'th', 't ', 's ', 'd ', ', ', 'ou', 'er', 'in', 'y ', 'an', ':\\n', 'or', 'o ', 'en', '\\n\\n', 'ar', ' th', 'on', 'll', 'ha', ',\\n', '.\\n\\n', 'is ', 'es', 'you', ' s', 'to ', 'and ', 'ow', 'ea', ' m', ' w', 'of', ' h', 'ing', 'om', ' a', 'ch', 'the ', 'st', ' b', 'no', 'ir', 'for', 've ', 'e, ', 'ith', ' the ', 'se', 'li', 'Th', 'll ', 're', 'st ', 'at ', 'An', 'I ', 'ear', 'im', 'it', 'oo', 'gh', 'at', 'is', 'le', 'er ', 'our', 'And ', \"'s \", 'ee', 'not ', 'my ', ';\\n', 'ra', '.\\n', 'your', 'ur', 'hat ', 'ri', 'ut ', 'ld ', 'of ', 'O:\\n', 'ed ', 'la', 'it ', 'ro', 'ere ', 'es ', 'd, ', 'un', 'EN', 'ke ', 'y, ', 'IN', ' d', 'as ', '?\\n\\n', 'fa', 'with', 'have ', 'S:\\n', ' c', 'Wh', 'that ', 'ent', 'the', 'ce', 'sh', 'ma', ' p', 'ther', 'be', '. ', 'AR', 'ce ', 'ing ', 'al', '; ', 'thou', 's, ', 'me ', 'se ', 'lo', 'ck', 'wh', 'il', \"'d \", 'IO:\\n', 'now', 'ill', 'be ', 'ell', 'rea', ' t', 't, ', 'ould ', 'e\\n', ' my ', 'ver', 'com', 'he ', ' to ', ' I', 'el', 'US:\\n', 'ol', 'di', ' g', 'ter', 'ay ', 'ain', ' you', 'The ', 'le ', 'ion', ' f', 'ru', 'if', 'em', 'and', 'To ', 'igh', 'are ', 'up', ',\\nAnd ', 'him', 'ed', 'ill ', 'ord', 'ich', 'ly ', 'ood ', 'UC', 'own', 'his ', 'ING', ' and ', 'con', 'ne', 'ay', 'e.\\n\\n', 'rom', 'id', 'us', 'AN', 'oun', 'man', 'ag', 'ER', 'OR', 'et ', 'res', 'sel', ' his ', 'e,\\n', 'et', 'ca', ' in', 'sha', '!\\n', 'ET', 'That ', 'po', 'qu', 'thy ', '!\\n\\n', 'mor', 'ul', 'no ', 'am', ' the', 'A:\\n', 'ven', 'by ', 's\\n', 'sp', 'KING', ' him', 'her', 'this ', ' this ', ' that ', '?\\n', 'oth', 'ong', 'But ', 'est ', 'o, ', 'but ', ' of', 'For', 'su', 'ut', ' with', 'one ', 'all', 'IC', 'end', 'OL', 'do ', 'I w', 'ome ', 's,\\n', 'sir', 'know', 'ct', ', and ', 'us ', 'self', ' st', 'ess ', 'EL', 'ake ', 'king']\n"
     ]
    }
   ],
   "source": [
    "def bpe(data, vocab):\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    bigrams = torch.concatenate([data[:-1].unsqueeze(1),data[1:].unsqueeze(1)], dim=1)\n",
    "    bigrams_id = bigrams @ torch.tensor([1,vocab_size])\n",
    "    bigrams_count = torch.zeros(vocab_size*vocab_size, dtype=torch.long).scatter_add(0, bigrams_id, torch.ones_like(bigrams_id))\n",
    "\n",
    "    most_popular_bigram = bigrams_count.argmax()\n",
    "\n",
    "    pos = ((bigrams_id == most_popular_bigram).nonzero(as_tuple=True)[0])\n",
    "\n",
    "    mask = data>=0\n",
    "    mask[pos] = False\n",
    "    data[pos+1] = vocab_size\n",
    "    p = (vocab[most_popular_bigram % vocab_size], vocab[most_popular_bigram//vocab_size])\n",
    "    #print(p)\n",
    "    vocab.append(\"%s%s\" % p)\n",
    "    return data[mask], vocab\n",
    "\n",
    "#text = \"abcdabab\"\n",
    "\n",
    "vocab = list(set(text))\n",
    "vocab.sort()\n",
    "\n",
    "t = Tokenizer(text)\n",
    "data = t.encode(text)\n",
    "\n",
    "print(data, vocab)\n",
    "\n",
    "for i in range(256):\n",
    "    data, vocab = bpe(data, vocab)\n",
    "    #print()\n",
    "\n",
    "print(len(data), vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join([ vocab[i] for i in data ]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
